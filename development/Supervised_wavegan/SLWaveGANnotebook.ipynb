{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cPickle as pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import xrange\n",
    "\n",
    "import loader\n",
    "from slwavegan import WaveGANGenerator, WaveGANDiscriminator\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Constants\n",
    "\"\"\"\n",
    "_FS = 16000\n",
    "_WINDOW_LEN = 16384\n",
    "_D_Z = 100\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  Trains a WaveGAN\n",
    "\"\"\"\n",
    "def train(fps, args):\n",
    "  with tf.name_scope('loader'):\n",
    "    x = loader.get_batch(fps, args.train_batch_size, _WINDOW_LEN, args.data_first_window)\n",
    "\n",
    "  # Make z vector\n",
    "  z = tf.random_uniform([args.train_batch_size, _D_Z], -1., 1., dtype=tf.float32)\n",
    "\n",
    "  # Make generator\n",
    "  with tf.variable_scope('G'):\n",
    "    G_z = WaveGANGenerator(z, train=True, **wavegan_g_kwargs)\n",
    "    if args.wavegan_genr_pp:\n",
    "      with tf.variable_scope('pp_filt'):\n",
    "        G_z = tf.layers.conv1d(G_z, 1, args.wavegan_genr_pp_len, use_bias=False, padding='same')\n",
    "  G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='G')\n",
    "\n",
    "  # Print G summary\n",
    "  print('-' * 80)\n",
    "  print('Generator vars')\n",
    "  nparams = 0\n",
    "  for v in G_vars:\n",
    "    v_shape = v.get_shape().as_list()\n",
    "    v_n = reduce(lambda x, y: x * y, v_shape)\n",
    "    nparams += v_n\n",
    "    print('{} ({}): {}'.format(v.get_shape().as_list(), v_n, v.name))\n",
    "  print('Total params: {} ({:.2f} MB)'.format(nparams, (float(nparams) * 4) / (1024 * 1024)))\n",
    "\n",
    "  # Summarize\n",
    "  tf.summary.audio('x', x, _FS)\n",
    "  tf.summary.audio('G_z', G_z, _FS)\n",
    "  G_z_rms = tf.sqrt(tf.reduce_mean(tf.square(G_z[:, :, 0]), axis=1))\n",
    "  x_rms = tf.sqrt(tf.reduce_mean(tf.square(x[:, :, 0]), axis=1))\n",
    "  tf.summary.histogram('x_rms_batch', x_rms)\n",
    "  tf.summary.histogram('G_z_rms_batch', G_z_rms)\n",
    "  tf.summary.scalar('x_rms', tf.reduce_mean(x_rms))\n",
    "  tf.summary.scalar('G_z_rms', tf.reduce_mean(G_z_rms))\n",
    "\n",
    "  # Make real discriminator\n",
    "  with tf.name_scope('D_x'), tf.variable_scope('D'):\n",
    "    D_x = WaveGANDiscriminator(x, **wavegan_d_kwargs)\n",
    "  D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D')\n",
    "\n",
    "  # Print D summary\n",
    "  print('-' * 80)\n",
    "  print('Discriminator vars')\n",
    "  nparams = 0\n",
    "  for v in D_vars:\n",
    "    v_shape = v.get_shape().as_list()\n",
    "    v_n = reduce(lambda x, y: x * y, v_shape)\n",
    "    nparams += v_n\n",
    "    print('{} ({}): {}'.format(v.get_shape().as_list(), v_n, v.name))\n",
    "  print('Total params: {} ({:.2f} MB)'.format(nparams, (float(nparams) * 4) / (1024 * 1024)))\n",
    "  print('-' * 80)\n",
    "\n",
    "  # Make fake discriminator\n",
    "  with tf.name_scope('D_G_z'), tf.variable_scope('D', reuse=True):\n",
    "    D_G_z = WaveGANDiscriminator(G_z, **wavegan_d_kwargs)\n",
    "\n",
    "  # Create loss\n",
    "  D_clip_weights = None\n",
    "  if args.wavegan_loss == 'dcgan':\n",
    "    fake = tf.zeros([args.train_batch_size], dtype=tf.float32)\n",
    "    real = tf.ones([args.train_batch_size], dtype=tf.float32)\n",
    "\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "      logits=D_G_z,\n",
    "      labels=real\n",
    "    ))\n",
    "\n",
    "    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "      logits=D_G_z,\n",
    "      labels=fake\n",
    "    ))\n",
    "    D_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "      logits=D_x,\n",
    "      labels=real\n",
    "    ))\n",
    "\n",
    "    D_loss /= 2.\n",
    "  elif args.wavegan_loss == 'lsgan':\n",
    "    G_loss = tf.reduce_mean((D_G_z - 1.) ** 2)\n",
    "    D_loss = tf.reduce_mean((D_x - 1.) ** 2)\n",
    "    D_loss += tf.reduce_mean(D_G_z ** 2)\n",
    "    D_loss /= 2.\n",
    "  elif args.wavegan_loss == 'wgan':\n",
    "    G_loss = -tf.reduce_mean(D_G_z)\n",
    "    D_loss = tf.reduce_mean(D_G_z) - tf.reduce_mean(D_x)\n",
    "\n",
    "    with tf.name_scope('D_clip_weights'):\n",
    "      clip_ops = []\n",
    "      for var in D_vars:\n",
    "        clip_bounds = [-.01, .01]\n",
    "        clip_ops.append(\n",
    "          tf.assign(\n",
    "            var,\n",
    "            tf.clip_by_value(var, clip_bounds[0], clip_bounds[1])\n",
    "          )\n",
    "        )\n",
    "      D_clip_weights = tf.group(*clip_ops)\n",
    "  elif args.wavegan_loss == 'wgan-gp':\n",
    "    G_loss = -tf.reduce_mean(D_G_z)\n",
    "    D_loss = tf.reduce_mean(D_G_z) - tf.reduce_mean(D_x)\n",
    "\n",
    "    alpha = tf.random_uniform(shape=[args.train_batch_size, 1, 1], minval=0., maxval=1.)\n",
    "    differences = G_z - x\n",
    "    interpolates = x + (alpha * differences)\n",
    "    with tf.name_scope('D_interp'), tf.variable_scope('D', reuse=True):\n",
    "      D_interp = WaveGANDiscriminator(interpolates, **wavegan_d_kwargs)\n",
    "\n",
    "    LAMBDA = 10\n",
    "    gradients = tf.gradients(D_interp, [interpolates])[0]\n",
    "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1, 2]))\n",
    "    gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2.)\n",
    "    D_loss += LAMBDA * gradient_penalty\n",
    "  else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  tf.summary.scalar('G_loss', G_loss)\n",
    "  tf.summary.scalar('D_loss', D_loss)\n",
    "\n",
    "  # Create (recommended) optimizer\n",
    "  if args.wavegan_loss == 'dcgan':\n",
    "    G_opt = tf.train.AdamOptimizer(\n",
    "        learning_rate=2e-4,\n",
    "        beta1=0.5)\n",
    "    D_opt = tf.train.AdamOptimizer(\n",
    "        learning_rate=2e-4,\n",
    "        beta1=0.5)\n",
    "  elif args.wavegan_loss == 'lsgan':\n",
    "    G_opt = tf.train.RMSPropOptimizer(\n",
    "        learning_rate=1e-4)\n",
    "    D_opt = tf.train.RMSPropOptimizer(\n",
    "        learning_rate=1e-4)\n",
    "  elif args.wavegan_loss == 'wgan':\n",
    "    G_opt = tf.train.RMSPropOptimizer(\n",
    "        learning_rate=5e-5)\n",
    "    D_opt = tf.train.RMSPropOptimizer(\n",
    "        learning_rate=5e-5)\n",
    "  elif args.wavegan_loss == 'wgan-gp':\n",
    "    G_opt = tf.train.AdamOptimizer(\n",
    "        learning_rate=1e-4,\n",
    "        beta1=0.5,\n",
    "        beta2=0.9)\n",
    "    D_opt = tf.train.AdamOptimizer(\n",
    "        learning_rate=1e-4,\n",
    "        beta1=0.5,\n",
    "        beta2=0.9)\n",
    "  else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  # Create training ops\n",
    "  G_train_op = G_opt.minimize(G_loss, var_list=G_vars,\n",
    "      global_step=tf.train.get_or_create_global_step())\n",
    "  D_train_op = D_opt.minimize(D_loss, var_list=D_vars)\n",
    "\n",
    "  # Run training\n",
    "  with tf.train.MonitoredTrainingSession(\n",
    "      checkpoint_dir=args.train_dir,\n",
    "      save_checkpoint_secs=args.train_save_secs,\n",
    "      save_summaries_secs=args.train_summary_secs) as sess:\n",
    "    while True:\n",
    "      # Train discriminator\n",
    "      for i in xrange(args.wavegan_disc_nupdates):\n",
    "        sess.run(D_train_op)\n",
    "\n",
    "        # Enforce Lipschitz constraint for WGAN\n",
    "        if D_clip_weights is not None:\n",
    "          sess.run(D_clip_weights)\n",
    "\n",
    "      # Train generator\n",
    "      sess.run(G_train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(args):\n",
    "  infer_dir = os.path.join(args.train_dir, 'infer')\n",
    "  if not os.path.isdir(infer_dir):\n",
    "    os.makedirs(infer_dir)\n",
    "\n",
    "  # Subgraph that generates latent vectors\n",
    "  samp_z_n = tf.placeholder(tf.int32, [], name='samp_z_n')\n",
    "  samp_z = tf.random_uniform([samp_z_n, _D_Z], -1.0, 1.0, dtype=tf.float32, name='samp_z')\n",
    "\n",
    "  # Input zo\n",
    "  z = tf.placeholder(tf.float32, [None, _D_Z], name='z')\n",
    "  flat_pad = tf.placeholder(tf.int32, [], name='flat_pad')\n",
    "\n",
    "  # Execute generator\n",
    "  with tf.variable_scope('G'):\n",
    "    G_z = WaveGANGenerator(z, train=False, **wavegan_g_kwargs)\n",
    "    if args.wavegan_genr_pp:\n",
    "      with tf.variable_scope('pp_filt'):\n",
    "        G_z = tf.layers.conv1d(G_z, 1, args.wavegan_genr_pp_len, use_bias=False, padding='same')\n",
    "  G_z = tf.identity(G_z, name='G_z')\n",
    "\n",
    "  # Flatten batch\n",
    "  nch = int(G_z.get_shape()[-1])\n",
    "  G_z_padded = tf.pad(G_z, [[0, 0], [0, flat_pad], [0, 0]])\n",
    "  G_z_flat = tf.reshape(G_z_padded, [-1, nch], name='G_z_flat')\n",
    "\n",
    "  # Encode to int16\n",
    "  def float_to_int16(x, name=None):\n",
    "    x_int16 = x * 32767.\n",
    "    x_int16 = tf.clip_by_value(x_int16, -32767., 32767.)\n",
    "    x_int16 = tf.cast(x_int16, tf.int16, name=name)\n",
    "    return x_int16\n",
    "  G_z_int16 = float_to_int16(G_z, name='G_z_int16')\n",
    "  G_z_flat_int16 = float_to_int16(G_z_flat, name='G_z_flat_int16')\n",
    "\n",
    "  # Create saver\n",
    "  G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='G')\n",
    "  global_step = tf.train.get_or_create_global_step()\n",
    "  saver = tf.train.Saver(G_vars + [global_step])\n",
    "\n",
    "  # Export graph\n",
    "  tf.train.write_graph(tf.get_default_graph(), infer_dir, 'infer.pbtxt')\n",
    "\n",
    "  # Export MetaGraph\n",
    "  infer_metagraph_fp = os.path.join(infer_dir, 'infer.meta')\n",
    "  tf.train.export_meta_graph(\n",
    "      filename=infer_metagraph_fp,\n",
    "      clear_devices=True,\n",
    "      saver_def=saver.as_saver_def())\n",
    "\n",
    "  # Reset graph (in case training afterwards)\n",
    "  tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    train_dir='train',\n",
    "    mode='train',\n",
    "    data_dir='data/dev/train_tf',\n",
    "    data_first_window=False,\n",
    "    wavegan_kernel_len=25,\n",
    "    wavegan_dim=64,\n",
    "    wavegan_batchnorm=False,\n",
    "    wavegan_disc_nupdates=5,\n",
    "    wavegan_loss='wgan-gp',\n",
    "    wavegan_genr_upsample='zeros',\n",
    "    wavegan_genr_pp=False,\n",
    "    wavegan_genr_pp_len=512,\n",
    "    wavegan_disc_phaseshuffle=2,\n",
    "    train_batch_size=64,\n",
    "    train_save_secs=300,\n",
    "    train_summary_secs=120,\n",
    "    preview_n=32,\n",
    "    incept_metagraph_fp='./eval/inception/infer.meta',\n",
    "    incept_ckpt_fp='./eval/inception/best_acc-103005',\n",
    "    incept_n=5000,\n",
    "    incept_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavegan_g_kwargs = {\n",
    "      'kernel_len': args.wavegan_kernel_len,\n",
    "      'dim': args.wavegan_dim,\n",
    "      'use_batchnorm': args.wavegan_batchnorm,\n",
    "      'upsample': args.wavegan_genr_upsample\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavegan_d_kwargs = {\n",
    "      'kernel_len': args.wavegan_kernel_len,\n",
    "      'dim': args.wavegan_dim,\n",
    "      'use_batchnorm': args.wavegan_batchnorm,\n",
    "      'phaseshuffle_rad': args.wavegan_disc_phaseshuffle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode == 'train':\n",
    "  split = 'train'\n",
    "else:\n",
    "  split = None\n",
    "\n",
    "# Make train dir\n",
    "if not os.path.isdir(args.train_dir):\n",
    "    os.makedirs(args.train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Generator vars\n",
      "[100, 16384] (1638400): G/z_project/dense/kernel:0\n",
      "[16384] (16384): G/z_project/dense/bias:0\n",
      "[1, 25, 512, 1024] (13107200): G/upconv_0/conv2d_transpose/kernel:0\n",
      "[512] (512): G/upconv_0/conv2d_transpose/bias:0\n",
      "[1, 25, 256, 512] (3276800): G/upconv_1/conv2d_transpose/kernel:0\n",
      "[256] (256): G/upconv_1/conv2d_transpose/bias:0\n",
      "[1, 25, 128, 256] (819200): G/upconv_2/conv2d_transpose/kernel:0\n",
      "[128] (128): G/upconv_2/conv2d_transpose/bias:0\n",
      "[1, 25, 64, 128] (204800): G/upconv_3/conv2d_transpose/kernel:0\n",
      "[64] (64): G/upconv_3/conv2d_transpose/bias:0\n",
      "[1, 25, 1, 64] (1600): G/upconv_4/conv2d_transpose/kernel:0\n",
      "[1] (1): G/upconv_4/conv2d_transpose/bias:0\n",
      "Total params: 19065345 (72.73 MB)\n",
      "--------------------------------------------------------------------------------\n",
      "Discriminator vars\n",
      "[25, 1, 64] (1600): D/downconv_0/conv1d/kernel:0\n",
      "[64] (64): D/downconv_0/conv1d/bias:0\n",
      "[25, 64, 128] (204800): D/downconv_1/conv1d/kernel:0\n",
      "[128] (128): D/downconv_1/conv1d/bias:0\n",
      "[25, 128, 256] (819200): D/downconv_2/conv1d/kernel:0\n",
      "[256] (256): D/downconv_2/conv1d/bias:0\n",
      "[25, 256, 512] (3276800): D/downconv_3/conv1d/kernel:0\n",
      "[512] (512): D/downconv_3/conv1d/bias:0\n",
      "[25, 512, 1024] (13107200): D/downconv_4/conv1d/kernel:0\n",
      "[1024] (1024): D/downconv_4/conv1d/bias:0\n",
      "[16384, 1] (16384): D/output/dense/kernel:0\n",
      "[1] (1): D/output/dense/bias:0\n",
      "Total params: 17427969 (66.48 MB)\n",
      "--------------------------------------------------------------------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from train/model.ckpt-7157\n",
      "INFO:tensorflow:Saving checkpoints for 7157 into train/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.894606\n",
      "INFO:tensorflow:global_step/sec: 0.93535\n",
      "INFO:tensorflow:Saving checkpoints for 7433 into train/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.937993\n",
      "INFO:tensorflow:global_step/sec: 0.947189\n"
     ]
    }
   ],
   "source": [
    "if split is not None:\n",
    "  fps = glob.glob(os.path.join(args.data_dir, split) + '*.tfrecord')\n",
    "\n",
    "if args.mode == 'train':\n",
    "  infer(args)\n",
    "  train(fps, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
